{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzm3ba4yhVMb6nMTHrCvPZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bythyag/neural-networks/blob/main/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1TrFOLj1RQCy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# starting point: raw text input\n",
        "input_raw1 = \"Colin was here.\"\n",
        "input_raw2 = \"Colin went away subito.\""
      ],
      "metadata": {
        "id": "Imb31VyCRZJh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = [\"Colin\", \"was\", \"here\", \"went\", \"away\", \"[UNK]\", \"[EOS]\", \"[PAD]\"]\n",
        "n_vocab = len(vocabulary)"
      ],
      "metadata": {
        "id": "jBp0-McNRejH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization: sequence of tokens from the vocabulary\n",
        "input_tok1 = [\"Colin\", \"was\", \"here\", \"[EOS]\", \"[PAD]\"]\n",
        "input_tok2 = [\"Colin\", \"went\", \"away\", \"subito\", \"[EOS]\"]\n",
        "\n",
        "# indexing: represent input as tensor of indices\n",
        "input_ind1 = torch.tensor([0,1,2,6,7])\n",
        "input_ind2 = torch.tensor([0,3,4,5,6])"
      ],
      "metadata": {
        "id": "Xc7aqHXlRt4n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "sequence_length = 5\n",
        "\n",
        "batched_input = torch.stack([input_ind1, input_ind2])\n",
        "print(batched_input)\n",
        "\n",
        "# or we can use this:\n",
        "batched_input = torch.cat([input_ind1, input_ind2]).reshape(batch_size, sequence_length)\n",
        "print(batched_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag6SQBeZR2bh",
        "outputId": "65635b74-b954-469b-da2a-b506aea05815"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2, 6, 7],\n",
            "        [0, 3, 4, 5, 6]])\n",
            "tensor([[0, 1, 2, 6, 7],\n",
            "        [0, 3, 4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding: vector-representation of each word (here 3 element vector representation for each word in vocab)\n",
        "# needs parameter for the size of the embedding\n",
        "\n",
        "n_embbeding_size = 3\n",
        "embedding = nn.Embedding(n_vocab, n_embbeding_size)\n",
        "\n",
        "#the parameters are initialised randomly and can be adjusted during the training\n",
        "for p in embedding.parameters():\n",
        "    print(p)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SXpwG4kSl6l",
        "outputId": "c17cc58d-9e12-4010-8a93-c4800dee0186"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.4922,  0.9187, -0.3402],\n",
            "        [-1.4696,  0.6587, -0.1652],\n",
            "        [ 0.0319, -0.7485, -0.5190],\n",
            "        [ 0.4224,  0.1670,  0.6420],\n",
            "        [-0.2827,  0.4471,  0.1822],\n",
            "        [ 0.3597, -0.4901,  1.1842],\n",
            "        [-0.1385,  0.9766,  1.8635],\n",
            "        [-0.1711,  1.1492, -1.1834]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding(input_ind1)\n",
        "#embedding for â€œ[EOS] is [-0.1385,  0.9766,  1.8635]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVc6hW6BTE2v",
        "outputId": "ac90566b-9a6c-4b82-c863-6abaca69b52c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4922,  0.9187, -0.3402],\n",
              "        [-1.4696,  0.6587, -0.1652],\n",
              "        [ 0.0319, -0.7485, -0.5190],\n",
              "        [-0.1385,  0.9766,  1.8635],\n",
              "        [-0.1711,  1.1492, -1.1834]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the 'embedding' takes batched input\n",
        "input_embedding = embedding(batched_input)\n",
        "print(input_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XlsbqwCTlGV",
        "outputId": "866cd1ac-31b0-4b7b-930a-58dfe6abfb07"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.4922,  0.9187, -0.3402],\n",
            "         [-1.4696,  0.6587, -0.1652],\n",
            "         [ 0.0319, -0.7485, -0.5190],\n",
            "         [-0.1385,  0.9766,  1.8635],\n",
            "         [-0.1711,  1.1492, -1.1834]],\n",
            "\n",
            "        [[ 0.4922,  0.9187, -0.3402],\n",
            "         [ 0.4224,  0.1670,  0.6420],\n",
            "         [-0.2827,  0.4471,  0.1822],\n",
            "         [ 0.3597, -0.4901,  1.1842],\n",
            "         [-0.1385,  0.9766,  1.8635]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden_size = 4\n",
        "\n",
        "# instantiate an LSTM\n",
        "lstm = nn.LSTM(input_size  = n_embbeding_size,\n",
        "               hidden_size = n_hidden_size,  # size of hidden state\n",
        "               num_layers  = 2,              # number of stacked layers\n",
        "               batch_first = True            # expect input in format (batch, sequence, token)\n",
        "               )"
      ],
      "metadata": {
        "id": "7CNqogFOTrQU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm #it is just a single layer lmao"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWuN1KZXUiVj",
        "outputId": "3b22872c-c255-4e19-ac23-1c3c9e623d6e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(3, 4, num_layers=2, batch_first=True)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, (hidden, cell) = lstm(input = input_embedding)\n",
        "\n",
        "print(\"\\nLSTM embeddings in last layer for each word:\\n\", output)\n",
        "print(\"---\")\n",
        "print(\"\\nHidden state of last word for each layer:\\n\", hidden)\n",
        "print(\"---\")\n",
        "print(\"\\nCell state of last word for each layer:\\n\", cell)\n",
        "print(\"---\")\n",
        "print(print(\"The first one is Layer 1, second one is layer 2, there the first row is batch 1, second row is batch 2\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLpmSx2UUzfu",
        "outputId": "058bb1f0-6cdf-446c-b437-a80abcd0a57a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LSTM embeddings in last layer for each word:\n",
            " tensor([[[ 0.0837, -0.0807, -0.0312,  0.1637],\n",
            "         [ 0.1266, -0.1247, -0.0420,  0.2936],\n",
            "         [ 0.1810, -0.1779, -0.0865,  0.3208],\n",
            "         [ 0.1973, -0.2049, -0.0803,  0.3576],\n",
            "         [ 0.2229, -0.2326, -0.0998,  0.3593]],\n",
            "\n",
            "        [[ 0.0837, -0.0807, -0.0312,  0.1637],\n",
            "         [ 0.1433, -0.1437, -0.0539,  0.2488],\n",
            "         [ 0.1817, -0.1860, -0.0675,  0.3049],\n",
            "         [ 0.2094, -0.2160, -0.0793,  0.3325],\n",
            "         [ 0.2180, -0.2310, -0.0744,  0.3617]]], grad_fn=<TransposeBackward0>)\n",
            "---\n",
            "\n",
            "Hidden state of last word for each layer:\n",
            " tensor([[[ 0.1893,  0.1909, -0.0941,  0.1243],\n",
            "         [ 0.3339,  0.0964,  0.0149,  0.0789]],\n",
            "\n",
            "        [[ 0.2229, -0.2326, -0.0998,  0.3593],\n",
            "         [ 0.2180, -0.2310, -0.0744,  0.3617]]], grad_fn=<StackBackward0>)\n",
            "---\n",
            "\n",
            "Cell state of last word for each layer:\n",
            " tensor([[[ 0.6643,  0.2503, -0.1253,  0.4158],\n",
            "         [ 0.6021,  0.1408,  0.0358,  0.2091]],\n",
            "\n",
            "        [[ 0.5662, -0.5077, -0.1852,  0.7426],\n",
            "         [ 0.5573, -0.5086, -0.1409,  0.7439]]], grad_fn=<StackBackward0>)\n",
            "---\n",
            "The first one is Layer 1, second one is layer 2, there the first row is batch 1, second row is batch 2\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__ZslqBkXEv5",
        "outputId": "667a5a89-6e63-4b1b-9420-60604d923d12"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map the LSTM embeddings of the last word onto a vector\n",
        "# of the same length as the vocabulary size\n",
        "\n",
        "linear_map = nn.Linear(n_hidden_size, n_vocab)\n",
        "weights = linear_map(output)\n",
        "print(linear_map)\n",
        "print(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8pZb7DrWTDL",
        "outputId": "2deb89a4-87ba-40eb-dd08-e63f47831fa7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=4, out_features=8, bias=True)\n",
            "tensor([[[ 0.4399, -0.1575, -0.0394, -0.0152,  0.1835, -0.3252,  0.3059,\n",
            "           0.1831],\n",
            "         [ 0.4268, -0.2458, -0.0229, -0.0852,  0.2092, -0.3864,  0.3046,\n",
            "           0.1490],\n",
            "         [ 0.4575, -0.2866, -0.0530, -0.1003,  0.2315, -0.4173,  0.2870,\n",
            "           0.1638],\n",
            "         [ 0.4569, -0.3236, -0.0566, -0.1247,  0.2471, -0.4357,  0.2743,\n",
            "           0.1642],\n",
            "         [ 0.4751, -0.3400, -0.0763, -0.1271,  0.2587, -0.4462,  0.2617,\n",
            "           0.1781]],\n",
            "\n",
            "        [[ 0.4399, -0.1575, -0.0394, -0.0152,  0.1835, -0.3252,  0.3059,\n",
            "           0.1831],\n",
            "         [ 0.4528, -0.2381, -0.0547, -0.0651,  0.2160, -0.3749,  0.2855,\n",
            "           0.1816],\n",
            "         [ 0.4609, -0.2918, -0.0647, -0.0983,  0.2379, -0.4077,  0.2712,\n",
            "           0.1810],\n",
            "         [ 0.4710, -0.3241, -0.0767, -0.1156,  0.2528, -0.4265,  0.2593,\n",
            "           0.1861],\n",
            "         [ 0.4675, -0.3487, -0.0752, -0.1336,  0.2619, -0.4399,  0.2536,\n",
            "           0.1823]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get probabilities from output weights\n",
        "def next_word_probabilities(weights):\n",
        "    softmax = torch.nn.Softmax(dim=2)\n",
        "    return(softmax(weights).detach().numpy().round(4))\n",
        "\n",
        "# without any training: next word weights have high entropy\n",
        "print(next_word_probabilities(weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN1tqIvJWxyA",
        "outputId": "8e17a906-2869-465b-be7c-10aa4db3ea32"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.1757 0.0967 0.1088 0.1115 0.136  0.0818 0.1537 0.1359]\n",
            "  [0.1774 0.0905 0.1131 0.1063 0.1427 0.0787 0.157  0.1344]\n",
            "  [0.1835 0.0872 0.1101 0.105  0.1463 0.0765 0.1547 0.1368]\n",
            "  [0.1847 0.0846 0.1105 0.1032 0.1497 0.0756 0.1539 0.1378]\n",
            "  [0.188  0.0832 0.1083 0.1029 0.1514 0.0748 0.1518 0.1397]]\n",
            "\n",
            " [[0.1757 0.0967 0.1088 0.1115 0.136  0.0818 0.1537 0.1359]\n",
            "  [0.1807 0.0906 0.1088 0.1077 0.1426 0.079  0.1529 0.1378]\n",
            "  [0.1839 0.0866 0.1087 0.1051 0.1472 0.0772 0.1522 0.139 ]\n",
            "  [0.1866 0.0843 0.1079 0.1038 0.15   0.0761 0.151  0.1403]\n",
            "  [0.187  0.0827 0.1087 0.1025 0.1522 0.0755 0.151  0.1406]]]\n"
          ]
        }
      ]
    }
  ]
}