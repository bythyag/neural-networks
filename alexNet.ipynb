{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zyouHqPABXkO"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # First Convolutional Block\n",
        "            nn.Conv2d(3, 96, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Second Convolutional Block\n",
        "            nn.Conv2d(96, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Third Convolutional Block\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Fourth Convolutional Block\n",
        "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Fifth Convolutional Block\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Dynamic computation of flattened size\n",
        "        self._initialize_fc_layer(num_classes)\n",
        "\n",
        "    def _initialize_fc_layer(self, num_classes):\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.randn(1, 3, 32, 32)\n",
        "            feature_output = self.features(sample_input)\n",
        "            flattened_size = feature_output.view(1, -1).shape[1]\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(flattened_size, 4096),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(2048, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "Zoro3qhMBtLD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize Model, Loss, and Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AlexNet().to(device)\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)"
      ],
      "metadata": {
        "id": "rxw3DF0yB4dv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3e7225-f55f-4f30-dda1-67170cb1091a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 50\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(trainloader)}], Loss: {running_loss/100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_val_loss = val_loss / len(testloader)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Train Loss: {running_loss/len(trainloader):.4f}, '\n",
        "          f'Val Loss: {avg_val_loss:.4f}, '\n",
        "          f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    # Learning Rate Scheduling\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Save Best Model\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Test Accuracy: {best_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KShz83bDeYc",
        "outputId": "6173139e-98c8-4318-8b29-8eb2648b699a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step [100/1563], Loss: 1.5186\n",
            "Epoch [1/50], Step [200/1563], Loss: 1.4295\n",
            "Epoch [1/50], Step [300/1563], Loss: 1.4427\n",
            "Epoch [1/50], Step [400/1563], Loss: 1.3512\n",
            "Epoch [1/50], Step [500/1563], Loss: 1.3550\n",
            "Epoch [1/50], Step [600/1563], Loss: 1.2734\n",
            "Epoch [1/50], Step [700/1563], Loss: 1.2373\n",
            "Epoch [1/50], Step [800/1563], Loss: 1.2089\n",
            "Epoch [1/50], Step [900/1563], Loss: 1.1996\n",
            "Epoch [1/50], Step [1000/1563], Loss: 1.1912\n",
            "Epoch [1/50], Step [1100/1563], Loss: 1.2070\n",
            "Epoch [1/50], Step [1200/1563], Loss: 1.1507\n",
            "Epoch [1/50], Step [1300/1563], Loss: 1.1446\n",
            "Epoch [1/50], Step [1400/1563], Loss: 1.1481\n",
            "Epoch [1/50], Step [1500/1563], Loss: 1.0957\n",
            "Epoch [1/50], Train Loss: 0.0457, Val Loss: 1.1404, Accuracy: 58.43%\n",
            "Epoch [2/50], Step [100/1563], Loss: 1.0718\n",
            "Epoch [2/50], Step [200/1563], Loss: 1.0734\n",
            "Epoch [2/50], Step [300/1563], Loss: 1.0294\n",
            "Epoch [2/50], Step [400/1563], Loss: 1.0485\n",
            "Epoch [2/50], Step [500/1563], Loss: 1.0546\n",
            "Epoch [2/50], Step [600/1563], Loss: 1.0441\n",
            "Epoch [2/50], Step [700/1563], Loss: 0.9540\n",
            "Epoch [2/50], Step [800/1563], Loss: 1.0050\n",
            "Epoch [2/50], Step [900/1563], Loss: 0.9822\n",
            "Epoch [2/50], Step [1000/1563], Loss: 0.9405\n",
            "Epoch [2/50], Step [1100/1563], Loss: 0.9587\n",
            "Epoch [2/50], Step [1200/1563], Loss: 0.9208\n",
            "Epoch [2/50], Step [1300/1563], Loss: 0.9951\n",
            "Epoch [2/50], Step [1400/1563], Loss: 0.9255\n",
            "Epoch [2/50], Step [1500/1563], Loss: 0.9382\n",
            "Epoch [2/50], Train Loss: 0.0359, Val Loss: 0.8172, Accuracy: 71.15%\n",
            "Epoch [3/50], Step [100/1563], Loss: 0.9365\n",
            "Epoch [3/50], Step [200/1563], Loss: 0.8749\n",
            "Epoch [3/50], Step [300/1563], Loss: 0.8606\n",
            "Epoch [3/50], Step [400/1563], Loss: 0.8560\n",
            "Epoch [3/50], Step [500/1563], Loss: 0.8555\n",
            "Epoch [3/50], Step [600/1563], Loss: 0.8846\n",
            "Epoch [3/50], Step [700/1563], Loss: 0.8158\n",
            "Epoch [3/50], Step [800/1563], Loss: 0.8624\n",
            "Epoch [3/50], Step [900/1563], Loss: 0.8463\n",
            "Epoch [3/50], Step [1000/1563], Loss: 0.8434\n",
            "Epoch [3/50], Step [1100/1563], Loss: 0.8520\n",
            "Epoch [3/50], Step [1200/1563], Loss: 0.7976\n",
            "Epoch [3/50], Step [1300/1563], Loss: 0.7976\n",
            "Epoch [3/50], Step [1400/1563], Loss: 0.8119\n",
            "Epoch [3/50], Step [1500/1563], Loss: 0.8181\n",
            "Epoch [3/50], Train Loss: 0.0318, Val Loss: 0.7551, Accuracy: 74.25%\n",
            "Epoch [4/50], Step [100/1563], Loss: 0.7564\n",
            "Epoch [4/50], Step [200/1563], Loss: 0.8130\n",
            "Epoch [4/50], Step [300/1563], Loss: 0.7798\n",
            "Epoch [4/50], Step [400/1563], Loss: 0.7588\n",
            "Epoch [4/50], Step [500/1563], Loss: 0.7674\n",
            "Epoch [4/50], Step [600/1563], Loss: 0.7780\n",
            "Epoch [4/50], Step [700/1563], Loss: 0.7568\n",
            "Epoch [4/50], Step [800/1563], Loss: 0.7617\n",
            "Epoch [4/50], Step [900/1563], Loss: 0.7456\n",
            "Epoch [4/50], Step [1000/1563], Loss: 0.7545\n",
            "Epoch [4/50], Step [1100/1563], Loss: 0.7397\n",
            "Epoch [4/50], Step [1200/1563], Loss: 0.6877\n",
            "Epoch [4/50], Step [1300/1563], Loss: 0.6958\n",
            "Epoch [4/50], Step [1400/1563], Loss: 0.7169\n",
            "Epoch [4/50], Step [1500/1563], Loss: 0.7419\n",
            "Epoch [4/50], Train Loss: 0.0295, Val Loss: 0.6185, Accuracy: 78.90%\n",
            "Epoch [5/50], Step [100/1563], Loss: 0.6729\n",
            "Epoch [5/50], Step [200/1563], Loss: 0.6945\n",
            "Epoch [5/50], Step [300/1563], Loss: 0.7028\n",
            "Epoch [5/50], Step [400/1563], Loss: 0.6893\n",
            "Epoch [5/50], Step [500/1563], Loss: 0.6923\n",
            "Epoch [5/50], Step [600/1563], Loss: 0.6746\n",
            "Epoch [5/50], Step [700/1563], Loss: 0.6382\n",
            "Epoch [5/50], Step [800/1563], Loss: 0.7091\n",
            "Epoch [5/50], Step [900/1563], Loss: 0.6690\n",
            "Epoch [5/50], Step [1000/1563], Loss: 0.6667\n",
            "Epoch [5/50], Step [1100/1563], Loss: 0.6847\n",
            "Epoch [5/50], Step [1200/1563], Loss: 0.6733\n",
            "Epoch [5/50], Step [1300/1563], Loss: 0.6445\n",
            "Epoch [5/50], Step [1400/1563], Loss: 0.6663\n",
            "Epoch [5/50], Step [1500/1563], Loss: 0.6554\n",
            "Epoch [5/50], Train Loss: 0.0277, Val Loss: 0.5522, Accuracy: 80.79%\n",
            "Epoch [6/50], Step [100/1563], Loss: 0.6494\n",
            "Epoch [6/50], Step [200/1563], Loss: 0.5720\n",
            "Epoch [6/50], Step [300/1563], Loss: 0.6141\n",
            "Epoch [6/50], Step [400/1563], Loss: 0.6314\n",
            "Epoch [6/50], Step [500/1563], Loss: 0.6418\n",
            "Epoch [6/50], Step [600/1563], Loss: 0.6459\n",
            "Epoch [6/50], Step [700/1563], Loss: 0.6258\n",
            "Epoch [6/50], Step [800/1563], Loss: 0.5919\n",
            "Epoch [6/50], Step [900/1563], Loss: 0.6171\n",
            "Epoch [6/50], Step [1000/1563], Loss: 0.6089\n",
            "Epoch [6/50], Step [1100/1563], Loss: 0.6038\n",
            "Epoch [6/50], Step [1200/1563], Loss: 0.5785\n",
            "Epoch [6/50], Step [1300/1563], Loss: 0.6241\n",
            "Epoch [6/50], Step [1400/1563], Loss: 0.5899\n",
            "Epoch [6/50], Step [1500/1563], Loss: 0.6136\n",
            "Epoch [6/50], Train Loss: 0.0234, Val Loss: 0.4974, Accuracy: 83.24%\n",
            "Epoch [7/50], Step [100/1563], Loss: 0.5698\n",
            "Epoch [7/50], Step [200/1563], Loss: 0.6025\n",
            "Epoch [7/50], Step [300/1563], Loss: 0.5688\n",
            "Epoch [7/50], Step [400/1563], Loss: 0.5598\n",
            "Epoch [7/50], Step [500/1563], Loss: 0.5592\n",
            "Epoch [7/50], Step [600/1563], Loss: 0.6045\n",
            "Epoch [7/50], Step [700/1563], Loss: 0.5667\n",
            "Epoch [7/50], Step [800/1563], Loss: 0.5791\n",
            "Epoch [7/50], Step [900/1563], Loss: 0.5831\n",
            "Epoch [7/50], Step [1000/1563], Loss: 0.5681\n",
            "Epoch [7/50], Step [1100/1563], Loss: 0.5979\n",
            "Epoch [7/50], Step [1200/1563], Loss: 0.5548\n",
            "Epoch [7/50], Step [1300/1563], Loss: 0.5532\n",
            "Epoch [7/50], Step [1400/1563], Loss: 0.5406\n",
            "Epoch [7/50], Step [1500/1563], Loss: 0.5493\n",
            "Epoch [7/50], Train Loss: 0.0223, Val Loss: 0.5052, Accuracy: 82.74%\n",
            "Epoch [8/50], Step [100/1563], Loss: 0.4889\n",
            "Epoch [8/50], Step [200/1563], Loss: 0.5559\n",
            "Epoch [8/50], Step [300/1563], Loss: 0.5210\n",
            "Epoch [8/50], Step [400/1563], Loss: 0.5327\n",
            "Epoch [8/50], Step [500/1563], Loss: 0.5065\n",
            "Epoch [8/50], Step [600/1563], Loss: 0.5436\n",
            "Epoch [8/50], Step [700/1563], Loss: 0.5035\n",
            "Epoch [8/50], Step [800/1563], Loss: 0.5111\n",
            "Epoch [8/50], Step [900/1563], Loss: 0.5405\n",
            "Epoch [8/50], Step [1000/1563], Loss: 0.5045\n",
            "Epoch [8/50], Step [1100/1563], Loss: 0.5372\n",
            "Epoch [8/50], Step [1200/1563], Loss: 0.5293\n",
            "Epoch [8/50], Step [1300/1563], Loss: 0.5182\n",
            "Epoch [8/50], Step [1400/1563], Loss: 0.5117\n",
            "Epoch [8/50], Step [1500/1563], Loss: 0.5331\n",
            "Epoch [8/50], Train Loss: 0.0207, Val Loss: 0.5006, Accuracy: 83.06%\n",
            "Epoch [9/50], Step [100/1563], Loss: 0.4904\n",
            "Epoch [9/50], Step [200/1563], Loss: 0.5022\n",
            "Epoch [9/50], Step [300/1563], Loss: 0.4718\n",
            "Epoch [9/50], Step [400/1563], Loss: 0.4949\n",
            "Epoch [9/50], Step [500/1563], Loss: 0.4694\n",
            "Epoch [9/50], Step [600/1563], Loss: 0.4884\n",
            "Epoch [9/50], Step [700/1563], Loss: 0.4709\n",
            "Epoch [9/50], Step [800/1563], Loss: 0.4738\n",
            "Epoch [9/50], Step [900/1563], Loss: 0.4901\n",
            "Epoch [9/50], Step [1000/1563], Loss: 0.4927\n",
            "Epoch [9/50], Step [1100/1563], Loss: 0.4978\n",
            "Epoch [9/50], Step [1200/1563], Loss: 0.4952\n",
            "Epoch [9/50], Step [1300/1563], Loss: 0.4916\n",
            "Epoch [9/50], Step [1400/1563], Loss: 0.4608\n",
            "Epoch [9/50], Step [1500/1563], Loss: 0.4877\n",
            "Epoch [9/50], Train Loss: 0.0201, Val Loss: 0.4633, Accuracy: 83.88%\n",
            "Epoch [10/50], Step [100/1563], Loss: 0.4525\n",
            "Epoch [10/50], Step [200/1563], Loss: 0.4381\n",
            "Epoch [10/50], Step [300/1563], Loss: 0.4821\n",
            "Epoch [10/50], Step [400/1563], Loss: 0.4877\n",
            "Epoch [10/50], Step [500/1563], Loss: 0.4446\n",
            "Epoch [10/50], Step [600/1563], Loss: 0.4358\n",
            "Epoch [10/50], Step [700/1563], Loss: 0.4494\n",
            "Epoch [10/50], Step [800/1563], Loss: 0.4507\n",
            "Epoch [10/50], Step [900/1563], Loss: 0.4733\n",
            "Epoch [10/50], Step [1000/1563], Loss: 0.4384\n",
            "Epoch [10/50], Step [1100/1563], Loss: 0.4429\n",
            "Epoch [10/50], Step [1200/1563], Loss: 0.4655\n",
            "Epoch [10/50], Step [1300/1563], Loss: 0.4599\n",
            "Epoch [10/50], Step [1400/1563], Loss: 0.4736\n",
            "Epoch [10/50], Step [1500/1563], Loss: 0.4159\n",
            "Epoch [10/50], Train Loss: 0.0174, Val Loss: 0.4579, Accuracy: 84.50%\n",
            "Epoch [11/50], Step [100/1563], Loss: 0.4289\n",
            "Epoch [11/50], Step [200/1563], Loss: 0.3989\n",
            "Epoch [11/50], Step [300/1563], Loss: 0.4276\n",
            "Epoch [11/50], Step [400/1563], Loss: 0.4488\n",
            "Epoch [11/50], Step [500/1563], Loss: 0.4428\n",
            "Epoch [11/50], Step [600/1563], Loss: 0.4632\n",
            "Epoch [11/50], Step [700/1563], Loss: 0.4132\n",
            "Epoch [11/50], Step [800/1563], Loss: 0.4176\n",
            "Epoch [11/50], Step [900/1563], Loss: 0.4163\n",
            "Epoch [11/50], Step [1000/1563], Loss: 0.4602\n",
            "Epoch [11/50], Step [1100/1563], Loss: 0.4186\n",
            "Epoch [11/50], Step [1200/1563], Loss: 0.4187\n",
            "Epoch [11/50], Step [1300/1563], Loss: 0.4512\n",
            "Epoch [11/50], Step [1400/1563], Loss: 0.4153\n",
            "Epoch [11/50], Step [1500/1563], Loss: 0.4326\n",
            "Epoch [11/50], Train Loss: 0.0166, Val Loss: 0.4391, Accuracy: 85.03%\n",
            "Epoch [12/50], Step [100/1563], Loss: 0.3424\n",
            "Epoch [12/50], Step [200/1563], Loss: 0.4057\n",
            "Epoch [12/50], Step [300/1563], Loss: 0.4275\n",
            "Epoch [12/50], Step [400/1563], Loss: 0.3955\n",
            "Epoch [12/50], Step [500/1563], Loss: 0.4173\n",
            "Epoch [12/50], Step [600/1563], Loss: 0.4134\n",
            "Epoch [12/50], Step [700/1563], Loss: 0.4186\n",
            "Epoch [12/50], Step [800/1563], Loss: 0.3924\n",
            "Epoch [12/50], Step [900/1563], Loss: 0.3896\n",
            "Epoch [12/50], Step [1000/1563], Loss: 0.3914\n",
            "Epoch [12/50], Step [1100/1563], Loss: 0.3745\n",
            "Epoch [12/50], Step [1200/1563], Loss: 0.3977\n",
            "Epoch [12/50], Step [1300/1563], Loss: 0.4103\n",
            "Epoch [12/50], Step [1400/1563], Loss: 0.4160\n",
            "Epoch [12/50], Step [1500/1563], Loss: 0.4241\n",
            "Epoch [12/50], Train Loss: 0.0164, Val Loss: 0.4457, Accuracy: 84.78%\n",
            "Epoch [13/50], Step [100/1563], Loss: 0.3341\n",
            "Epoch [13/50], Step [200/1563], Loss: 0.3760\n",
            "Epoch [13/50], Step [300/1563], Loss: 0.3878\n",
            "Epoch [13/50], Step [400/1563], Loss: 0.3924\n",
            "Epoch [13/50], Step [500/1563], Loss: 0.3715\n",
            "Epoch [13/50], Step [600/1563], Loss: 0.3937\n",
            "Epoch [13/50], Step [700/1563], Loss: 0.3917\n",
            "Epoch [13/50], Step [800/1563], Loss: 0.3609\n",
            "Epoch [13/50], Step [900/1563], Loss: 0.3942\n",
            "Epoch [13/50], Step [1000/1563], Loss: 0.3734\n",
            "Epoch [13/50], Step [1100/1563], Loss: 0.3961\n",
            "Epoch [13/50], Step [1200/1563], Loss: 0.3686\n",
            "Epoch [13/50], Step [1300/1563], Loss: 0.3921\n",
            "Epoch [13/50], Step [1400/1563], Loss: 0.3723\n",
            "Epoch [13/50], Step [1500/1563], Loss: 0.3822\n",
            "Epoch [13/50], Train Loss: 0.0155, Val Loss: 0.3959, Accuracy: 86.53%\n",
            "Epoch [14/50], Step [100/1563], Loss: 0.3468\n",
            "Epoch [14/50], Step [200/1563], Loss: 0.3563\n",
            "Epoch [14/50], Step [300/1563], Loss: 0.3636\n",
            "Epoch [14/50], Step [400/1563], Loss: 0.3615\n",
            "Epoch [14/50], Step [500/1563], Loss: 0.3473\n",
            "Epoch [14/50], Step [600/1563], Loss: 0.3883\n",
            "Epoch [14/50], Step [700/1563], Loss: 0.3372\n",
            "Epoch [14/50], Step [800/1563], Loss: 0.3408\n",
            "Epoch [14/50], Step [900/1563], Loss: 0.3749\n",
            "Epoch [14/50], Step [1000/1563], Loss: 0.3555\n",
            "Epoch [14/50], Step [1100/1563], Loss: 0.3608\n",
            "Epoch [14/50], Step [1200/1563], Loss: 0.3596\n",
            "Epoch [14/50], Step [1300/1563], Loss: 0.3569\n",
            "Epoch [14/50], Step [1400/1563], Loss: 0.4103\n",
            "Epoch [14/50], Step [1500/1563], Loss: 0.3807\n",
            "Epoch [14/50], Train Loss: 0.0143, Val Loss: 0.4002, Accuracy: 86.68%\n",
            "Epoch [15/50], Step [100/1563], Loss: 0.3447\n",
            "Epoch [15/50], Step [200/1563], Loss: 0.3262\n",
            "Epoch [15/50], Step [300/1563], Loss: 0.3177\n",
            "Epoch [15/50], Step [400/1563], Loss: 0.3448\n",
            "Epoch [15/50], Step [500/1563], Loss: 0.3389\n",
            "Epoch [15/50], Step [600/1563], Loss: 0.3260\n",
            "Epoch [15/50], Step [700/1563], Loss: 0.3444\n",
            "Epoch [15/50], Step [800/1563], Loss: 0.3469\n",
            "Epoch [15/50], Step [900/1563], Loss: 0.3618\n",
            "Epoch [15/50], Step [1000/1563], Loss: 0.3440\n",
            "Epoch [15/50], Step [1100/1563], Loss: 0.3640\n",
            "Epoch [15/50], Step [1200/1563], Loss: 0.3119\n",
            "Epoch [15/50], Step [1300/1563], Loss: 0.3377\n",
            "Epoch [15/50], Step [1400/1563], Loss: 0.3300\n",
            "Epoch [15/50], Step [1500/1563], Loss: 0.3859\n",
            "Epoch [15/50], Train Loss: 0.0147, Val Loss: 0.3779, Accuracy: 87.15%\n",
            "Epoch [16/50], Step [100/1563], Loss: 0.3410\n",
            "Epoch [16/50], Step [200/1563], Loss: 0.3039\n",
            "Epoch [16/50], Step [300/1563], Loss: 0.3452\n",
            "Epoch [16/50], Step [400/1563], Loss: 0.3250\n",
            "Epoch [16/50], Step [500/1563], Loss: 0.3406\n",
            "Epoch [16/50], Step [600/1563], Loss: 0.3168\n",
            "Epoch [16/50], Step [700/1563], Loss: 0.3248\n",
            "Epoch [16/50], Step [800/1563], Loss: 0.3062\n",
            "Epoch [16/50], Step [900/1563], Loss: 0.3337\n",
            "Epoch [16/50], Step [1000/1563], Loss: 0.3225\n",
            "Epoch [16/50], Step [1100/1563], Loss: 0.3528\n",
            "Epoch [16/50], Step [1200/1563], Loss: 0.3126\n",
            "Epoch [16/50], Step [1300/1563], Loss: 0.3432\n",
            "Epoch [16/50], Step [1400/1563], Loss: 0.3352\n",
            "Epoch [16/50], Step [1500/1563], Loss: 0.3534\n",
            "Epoch [16/50], Train Loss: 0.0123, Val Loss: 0.3932, Accuracy: 86.74%\n",
            "Epoch [17/50], Step [100/1563], Loss: 0.3233\n",
            "Epoch [17/50], Step [200/1563], Loss: 0.2948\n",
            "Epoch [17/50], Step [300/1563], Loss: 0.3041\n",
            "Epoch [17/50], Step [400/1563], Loss: 0.3082\n",
            "Epoch [17/50], Step [500/1563], Loss: 0.3125\n",
            "Epoch [17/50], Step [600/1563], Loss: 0.3017\n",
            "Epoch [17/50], Step [700/1563], Loss: 0.3262\n",
            "Epoch [17/50], Step [800/1563], Loss: 0.3304\n",
            "Epoch [17/50], Step [900/1563], Loss: 0.2939\n",
            "Epoch [17/50], Step [1000/1563], Loss: 0.2995\n",
            "Epoch [17/50], Step [1100/1563], Loss: 0.3101\n",
            "Epoch [17/50], Step [1200/1563], Loss: 0.2949\n",
            "Epoch [17/50], Step [1300/1563], Loss: 0.3031\n",
            "Epoch [17/50], Step [1400/1563], Loss: 0.3043\n",
            "Epoch [17/50], Step [1500/1563], Loss: 0.3241\n",
            "Epoch [17/50], Train Loss: 0.0133, Val Loss: 0.3612, Accuracy: 88.25%\n",
            "Epoch [18/50], Step [100/1563], Loss: 0.2638\n",
            "Epoch [18/50], Step [200/1563], Loss: 0.3014\n",
            "Epoch [18/50], Step [300/1563], Loss: 0.2629\n",
            "Epoch [18/50], Step [400/1563], Loss: 0.2791\n",
            "Epoch [18/50], Step [500/1563], Loss: 0.2941\n",
            "Epoch [18/50], Step [600/1563], Loss: 0.3015\n",
            "Epoch [18/50], Step [700/1563], Loss: 0.3167\n",
            "Epoch [18/50], Step [800/1563], Loss: 0.3128\n",
            "Epoch [18/50], Step [900/1563], Loss: 0.3113\n",
            "Epoch [18/50], Step [1000/1563], Loss: 0.3165\n",
            "Epoch [18/50], Step [1100/1563], Loss: 0.2869\n",
            "Epoch [18/50], Step [1200/1563], Loss: 0.3171\n",
            "Epoch [18/50], Step [1300/1563], Loss: 0.2894\n",
            "Epoch [18/50], Step [1400/1563], Loss: 0.3319\n",
            "Epoch [18/50], Step [1500/1563], Loss: 0.3178\n",
            "Epoch [18/50], Train Loss: 0.0124, Val Loss: 0.3687, Accuracy: 87.86%\n",
            "Epoch [19/50], Step [100/1563], Loss: 0.2326\n",
            "Epoch [19/50], Step [200/1563], Loss: 0.2579\n",
            "Epoch [19/50], Step [300/1563], Loss: 0.2855\n",
            "Epoch [19/50], Step [400/1563], Loss: 0.2745\n",
            "Epoch [19/50], Step [500/1563], Loss: 0.2923\n",
            "Epoch [19/50], Step [600/1563], Loss: 0.2410\n",
            "Epoch [19/50], Step [700/1563], Loss: 0.2701\n",
            "Epoch [19/50], Step [800/1563], Loss: 0.2830\n",
            "Epoch [19/50], Step [900/1563], Loss: 0.3196\n",
            "Epoch [19/50], Step [1000/1563], Loss: 0.3034\n",
            "Epoch [19/50], Step [1100/1563], Loss: 0.2964\n",
            "Epoch [19/50], Step [1200/1563], Loss: 0.2981\n",
            "Epoch [19/50], Step [1300/1563], Loss: 0.2900\n",
            "Epoch [19/50], Step [1400/1563], Loss: 0.2915\n",
            "Epoch [19/50], Step [1500/1563], Loss: 0.2728\n",
            "Epoch [19/50], Train Loss: 0.0127, Val Loss: 0.3581, Accuracy: 88.21%\n",
            "Epoch [20/50], Step [100/1563], Loss: 0.2453\n",
            "Epoch [20/50], Step [200/1563], Loss: 0.2659\n",
            "Epoch [20/50], Step [300/1563], Loss: 0.2594\n",
            "Epoch [20/50], Step [400/1563], Loss: 0.2485\n",
            "Epoch [20/50], Step [500/1563], Loss: 0.2670\n",
            "Epoch [20/50], Step [600/1563], Loss: 0.2992\n",
            "Epoch [20/50], Step [700/1563], Loss: 0.2992\n",
            "Epoch [20/50], Step [800/1563], Loss: 0.2566\n",
            "Epoch [20/50], Step [900/1563], Loss: 0.2532\n",
            "Epoch [20/50], Step [1000/1563], Loss: 0.2962\n",
            "Epoch [20/50], Step [1100/1563], Loss: 0.3101\n",
            "Epoch [20/50], Step [1200/1563], Loss: 0.2458\n",
            "Epoch [20/50], Step [1300/1563], Loss: 0.2597\n",
            "Epoch [20/50], Step [1400/1563], Loss: 0.2824\n",
            "Epoch [20/50], Step [1500/1563], Loss: 0.2979\n",
            "Epoch [20/50], Train Loss: 0.0118, Val Loss: 0.3778, Accuracy: 87.40%\n",
            "Epoch [21/50], Step [100/1563], Loss: 0.2245\n",
            "Epoch [21/50], Step [200/1563], Loss: 0.2335\n",
            "Epoch [21/50], Step [300/1563], Loss: 0.2455\n",
            "Epoch [21/50], Step [400/1563], Loss: 0.2614\n",
            "Epoch [21/50], Step [500/1563], Loss: 0.2728\n",
            "Epoch [21/50], Step [600/1563], Loss: 0.2475\n",
            "Epoch [21/50], Step [700/1563], Loss: 0.2456\n",
            "Epoch [21/50], Step [800/1563], Loss: 0.2884\n",
            "Epoch [21/50], Step [900/1563], Loss: 0.2663\n",
            "Epoch [21/50], Step [1000/1563], Loss: 0.2665\n",
            "Epoch [21/50], Step [1100/1563], Loss: 0.2531\n",
            "Epoch [21/50], Step [1200/1563], Loss: 0.2853\n",
            "Epoch [21/50], Step [1300/1563], Loss: 0.2727\n",
            "Epoch [21/50], Step [1400/1563], Loss: 0.2565\n",
            "Epoch [21/50], Step [1500/1563], Loss: 0.2776\n",
            "Epoch [21/50], Train Loss: 0.0119, Val Loss: 0.3828, Accuracy: 87.78%\n",
            "Epoch [22/50], Step [100/1563], Loss: 0.2516\n",
            "Epoch [22/50], Step [200/1563], Loss: 0.2262\n",
            "Epoch [22/50], Step [300/1563], Loss: 0.2457\n",
            "Epoch [22/50], Step [400/1563], Loss: 0.2364\n",
            "Epoch [22/50], Step [500/1563], Loss: 0.2733\n",
            "Epoch [22/50], Step [600/1563], Loss: 0.2600\n",
            "Epoch [22/50], Step [700/1563], Loss: 0.2499\n",
            "Epoch [22/50], Step [800/1563], Loss: 0.2841\n",
            "Epoch [22/50], Step [900/1563], Loss: 0.2499\n",
            "Epoch [22/50], Step [1000/1563], Loss: 0.2549\n",
            "Epoch [22/50], Step [1100/1563], Loss: 0.2662\n",
            "Epoch [22/50], Step [1200/1563], Loss: 0.2584\n",
            "Epoch [22/50], Step [1300/1563], Loss: 0.2654\n",
            "Epoch [22/50], Step [1400/1563], Loss: 0.2598\n",
            "Epoch [22/50], Step [1500/1563], Loss: 0.2616\n",
            "Epoch [22/50], Train Loss: 0.0109, Val Loss: 0.4086, Accuracy: 87.34%\n",
            "Epoch [23/50], Step [100/1563], Loss: 0.2160\n",
            "Epoch [23/50], Step [200/1563], Loss: 0.2131\n",
            "Epoch [23/50], Step [300/1563], Loss: 0.2150\n",
            "Epoch [23/50], Step [400/1563], Loss: 0.2463\n",
            "Epoch [23/50], Step [500/1563], Loss: 0.2919\n",
            "Epoch [23/50], Step [600/1563], Loss: 0.2439\n",
            "Epoch [23/50], Step [700/1563], Loss: 0.2555\n",
            "Epoch [23/50], Step [800/1563], Loss: 0.2471\n",
            "Epoch [23/50], Step [900/1563], Loss: 0.2325\n",
            "Epoch [23/50], Step [1000/1563], Loss: 0.2647\n",
            "Epoch [23/50], Step [1100/1563], Loss: 0.2409\n",
            "Epoch [23/50], Step [1200/1563], Loss: 0.2499\n",
            "Epoch [23/50], Step [1300/1563], Loss: 0.2746\n",
            "Epoch [23/50], Step [1400/1563], Loss: 0.2209\n",
            "Epoch [23/50], Step [1500/1563], Loss: 0.2590\n",
            "Epoch [23/50], Train Loss: 0.0092, Val Loss: 0.3930, Accuracy: 87.61%\n",
            "Epoch [24/50], Step [100/1563], Loss: 0.2047\n",
            "Epoch [24/50], Step [200/1563], Loss: 0.1757\n",
            "Epoch [24/50], Step [300/1563], Loss: 0.1676\n",
            "Epoch [24/50], Step [400/1563], Loss: 0.1498\n",
            "Epoch [24/50], Step [500/1563], Loss: 0.1592\n",
            "Epoch [24/50], Step [600/1563], Loss: 0.1647\n",
            "Epoch [24/50], Step [700/1563], Loss: 0.1532\n",
            "Epoch [24/50], Step [800/1563], Loss: 0.1451\n",
            "Epoch [24/50], Step [900/1563], Loss: 0.1709\n",
            "Epoch [24/50], Step [1000/1563], Loss: 0.1550\n",
            "Epoch [24/50], Step [1100/1563], Loss: 0.1488\n",
            "Epoch [24/50], Step [1200/1563], Loss: 0.1571\n",
            "Epoch [24/50], Step [1300/1563], Loss: 0.1435\n",
            "Epoch [24/50], Step [1400/1563], Loss: 0.1453\n",
            "Epoch [24/50], Step [1500/1563], Loss: 0.1593\n",
            "Epoch [24/50], Train Loss: 0.0054, Val Loss: 0.2971, Accuracy: 90.48%\n",
            "Epoch [25/50], Step [100/1563], Loss: 0.1341\n",
            "Epoch [25/50], Step [200/1563], Loss: 0.1424\n",
            "Epoch [25/50], Step [300/1563], Loss: 0.1315\n",
            "Epoch [25/50], Step [400/1563], Loss: 0.1249\n",
            "Epoch [25/50], Step [500/1563], Loss: 0.1478\n",
            "Epoch [25/50], Step [600/1563], Loss: 0.1428\n",
            "Epoch [25/50], Step [700/1563], Loss: 0.1282\n",
            "Epoch [25/50], Step [800/1563], Loss: 0.1223\n",
            "Epoch [25/50], Step [900/1563], Loss: 0.1319\n",
            "Epoch [25/50], Step [1000/1563], Loss: 0.1281\n",
            "Epoch [25/50], Step [1100/1563], Loss: 0.1219\n",
            "Epoch [25/50], Step [1200/1563], Loss: 0.1174\n",
            "Epoch [25/50], Step [1300/1563], Loss: 0.1198\n",
            "Epoch [25/50], Step [1400/1563], Loss: 0.1266\n",
            "Epoch [25/50], Step [1500/1563], Loss: 0.1206\n",
            "Epoch [25/50], Train Loss: 0.0045, Val Loss: 0.2924, Accuracy: 90.78%\n",
            "Epoch [26/50], Step [100/1563], Loss: 0.1090\n",
            "Epoch [26/50], Step [200/1563], Loss: 0.0992\n",
            "Epoch [26/50], Step [300/1563], Loss: 0.1092\n",
            "Epoch [26/50], Step [400/1563], Loss: 0.1180\n",
            "Epoch [26/50], Step [500/1563], Loss: 0.1155\n",
            "Epoch [26/50], Step [600/1563], Loss: 0.1090\n",
            "Epoch [26/50], Step [700/1563], Loss: 0.1225\n",
            "Epoch [26/50], Step [800/1563], Loss: 0.1317\n",
            "Epoch [26/50], Step [900/1563], Loss: 0.1004\n",
            "Epoch [26/50], Step [1000/1563], Loss: 0.1136\n",
            "Epoch [26/50], Step [1100/1563], Loss: 0.1171\n",
            "Epoch [26/50], Step [1200/1563], Loss: 0.1031\n",
            "Epoch [26/50], Step [1300/1563], Loss: 0.0993\n",
            "Epoch [26/50], Step [1400/1563], Loss: 0.1161\n",
            "Epoch [26/50], Step [1500/1563], Loss: 0.1285\n",
            "Epoch [26/50], Train Loss: 0.0044, Val Loss: 0.3008, Accuracy: 90.96%\n",
            "Epoch [27/50], Step [100/1563], Loss: 0.0994\n",
            "Epoch [27/50], Step [200/1563], Loss: 0.1066\n",
            "Epoch [27/50], Step [300/1563], Loss: 0.1251\n",
            "Epoch [27/50], Step [400/1563], Loss: 0.1003\n",
            "Epoch [27/50], Step [500/1563], Loss: 0.1091\n",
            "Epoch [27/50], Step [600/1563], Loss: 0.1009\n",
            "Epoch [27/50], Step [700/1563], Loss: 0.1110\n",
            "Epoch [27/50], Step [800/1563], Loss: 0.1102\n",
            "Epoch [27/50], Step [900/1563], Loss: 0.1139\n",
            "Epoch [27/50], Step [1000/1563], Loss: 0.0904\n",
            "Epoch [27/50], Step [1100/1563], Loss: 0.1085\n",
            "Epoch [27/50], Step [1200/1563], Loss: 0.1094\n",
            "Epoch [27/50], Step [1300/1563], Loss: 0.0877\n",
            "Epoch [27/50], Step [1400/1563], Loss: 0.1015\n",
            "Epoch [27/50], Step [1500/1563], Loss: 0.0972\n",
            "Epoch [27/50], Train Loss: 0.0038, Val Loss: 0.3124, Accuracy: 90.59%\n",
            "Epoch [28/50], Step [100/1563], Loss: 0.0806\n",
            "Epoch [28/50], Step [200/1563], Loss: 0.1051\n",
            "Epoch [28/50], Step [300/1563], Loss: 0.1055\n",
            "Epoch [28/50], Step [400/1563], Loss: 0.0978\n",
            "Epoch [28/50], Step [500/1563], Loss: 0.0855\n",
            "Epoch [28/50], Step [600/1563], Loss: 0.1015\n",
            "Epoch [28/50], Step [700/1563], Loss: 0.1014\n",
            "Epoch [28/50], Step [800/1563], Loss: 0.0873\n",
            "Epoch [28/50], Step [900/1563], Loss: 0.0916\n",
            "Epoch [28/50], Step [1000/1563], Loss: 0.0854\n",
            "Epoch [28/50], Step [1100/1563], Loss: 0.1009\n",
            "Epoch [28/50], Step [1200/1563], Loss: 0.0928\n",
            "Epoch [28/50], Step [1300/1563], Loss: 0.0835\n",
            "Epoch [28/50], Step [1400/1563], Loss: 0.0928\n",
            "Epoch [28/50], Step [1500/1563], Loss: 0.0821\n",
            "Epoch [28/50], Train Loss: 0.0040, Val Loss: 0.3199, Accuracy: 90.58%\n",
            "Epoch [29/50], Step [100/1563], Loss: 0.0702\n",
            "Epoch [29/50], Step [200/1563], Loss: 0.0924\n",
            "Epoch [29/50], Step [300/1563], Loss: 0.0735\n",
            "Epoch [29/50], Step [400/1563], Loss: 0.0746\n",
            "Epoch [29/50], Step [500/1563], Loss: 0.0943\n",
            "Epoch [29/50], Step [600/1563], Loss: 0.0792\n",
            "Epoch [29/50], Step [700/1563], Loss: 0.0943\n",
            "Epoch [29/50], Step [800/1563], Loss: 0.0893\n",
            "Epoch [29/50], Step [900/1563], Loss: 0.0915\n",
            "Epoch [29/50], Step [1000/1563], Loss: 0.0906\n",
            "Epoch [29/50], Step [1100/1563], Loss: 0.0888\n",
            "Epoch [29/50], Step [1200/1563], Loss: 0.0854\n",
            "Epoch [29/50], Step [1300/1563], Loss: 0.1004\n",
            "Epoch [29/50], Step [1400/1563], Loss: 0.0960\n",
            "Epoch [29/50], Step [1500/1563], Loss: 0.0890\n",
            "Epoch [29/50], Train Loss: 0.0037, Val Loss: 0.3094, Accuracy: 90.91%\n",
            "Epoch [30/50], Step [100/1563], Loss: 0.0787\n",
            "Epoch [30/50], Step [200/1563], Loss: 0.0824\n",
            "Epoch [30/50], Step [300/1563], Loss: 0.0785\n",
            "Epoch [30/50], Step [400/1563], Loss: 0.0897\n",
            "Epoch [30/50], Step [500/1563], Loss: 0.0646\n",
            "Epoch [30/50], Step [600/1563], Loss: 0.0837\n",
            "Epoch [30/50], Step [700/1563], Loss: 0.0747\n",
            "Epoch [30/50], Step [800/1563], Loss: 0.0771\n",
            "Epoch [30/50], Step [900/1563], Loss: 0.0664\n",
            "Epoch [30/50], Step [1000/1563], Loss: 0.0806\n",
            "Epoch [30/50], Step [1100/1563], Loss: 0.0725\n",
            "Epoch [30/50], Step [1200/1563], Loss: 0.0847\n",
            "Epoch [30/50], Step [1300/1563], Loss: 0.0801\n",
            "Epoch [30/50], Step [1400/1563], Loss: 0.0706\n",
            "Epoch [30/50], Step [1500/1563], Loss: 0.0716\n",
            "Epoch [30/50], Train Loss: 0.0029, Val Loss: 0.3110, Accuracy: 90.91%\n",
            "Epoch [31/50], Step [100/1563], Loss: 0.0768\n",
            "Epoch [31/50], Step [200/1563], Loss: 0.0886\n",
            "Epoch [31/50], Step [300/1563], Loss: 0.0797\n",
            "Epoch [31/50], Step [400/1563], Loss: 0.0691\n",
            "Epoch [31/50], Step [500/1563], Loss: 0.0777\n",
            "Epoch [31/50], Step [600/1563], Loss: 0.0834\n",
            "Epoch [31/50], Step [700/1563], Loss: 0.0790\n",
            "Epoch [31/50], Step [800/1563], Loss: 0.0764\n",
            "Epoch [31/50], Step [900/1563], Loss: 0.0726\n",
            "Epoch [31/50], Step [1000/1563], Loss: 0.0777\n",
            "Epoch [31/50], Step [1100/1563], Loss: 0.0796\n",
            "Epoch [31/50], Step [1200/1563], Loss: 0.0860\n",
            "Epoch [31/50], Step [1300/1563], Loss: 0.0774\n",
            "Epoch [31/50], Step [1400/1563], Loss: 0.0750\n",
            "Epoch [31/50], Step [1500/1563], Loss: 0.0868\n",
            "Epoch [31/50], Train Loss: 0.0029, Val Loss: 0.3056, Accuracy: 90.92%\n",
            "Epoch [32/50], Step [100/1563], Loss: 0.0639\n",
            "Epoch [32/50], Step [200/1563], Loss: 0.0723\n",
            "Epoch [32/50], Step [300/1563], Loss: 0.0799\n",
            "Epoch [32/50], Step [400/1563], Loss: 0.0804\n",
            "Epoch [32/50], Step [500/1563], Loss: 0.0757\n",
            "Epoch [32/50], Step [600/1563], Loss: 0.0757\n",
            "Epoch [32/50], Step [700/1563], Loss: 0.0752\n",
            "Epoch [32/50], Step [800/1563], Loss: 0.0775\n",
            "Epoch [32/50], Step [900/1563], Loss: 0.0690\n",
            "Epoch [32/50], Step [1000/1563], Loss: 0.0818\n",
            "Epoch [32/50], Step [1100/1563], Loss: 0.0709\n",
            "Epoch [32/50], Step [1200/1563], Loss: 0.0796\n",
            "Epoch [32/50], Step [1300/1563], Loss: 0.0874\n",
            "Epoch [32/50], Step [1400/1563], Loss: 0.0749\n",
            "Epoch [32/50], Step [1500/1563], Loss: 0.0786\n",
            "Epoch [32/50], Train Loss: 0.0025, Val Loss: 0.3050, Accuracy: 91.07%\n",
            "Epoch [33/50], Step [100/1563], Loss: 0.0617\n",
            "Epoch [33/50], Step [200/1563], Loss: 0.0679\n",
            "Epoch [33/50], Step [300/1563], Loss: 0.0659\n",
            "Epoch [33/50], Step [400/1563], Loss: 0.0723\n",
            "Epoch [33/50], Step [500/1563], Loss: 0.0690\n",
            "Epoch [33/50], Step [600/1563], Loss: 0.0736\n",
            "Epoch [33/50], Step [700/1563], Loss: 0.0724\n",
            "Epoch [33/50], Step [800/1563], Loss: 0.0663\n",
            "Epoch [33/50], Step [900/1563], Loss: 0.0797\n",
            "Epoch [33/50], Step [1000/1563], Loss: 0.0719\n",
            "Epoch [33/50], Step [1100/1563], Loss: 0.0718\n",
            "Epoch [33/50], Step [1200/1563], Loss: 0.0614\n",
            "Epoch [33/50], Step [1300/1563], Loss: 0.0718\n",
            "Epoch [33/50], Step [1400/1563], Loss: 0.0712\n",
            "Epoch [33/50], Step [1500/1563], Loss: 0.0696\n",
            "Epoch [33/50], Train Loss: 0.0028, Val Loss: 0.3063, Accuracy: 90.98%\n",
            "Epoch [34/50], Step [100/1563], Loss: 0.0842\n",
            "Epoch [34/50], Step [200/1563], Loss: 0.0694\n",
            "Epoch [34/50], Step [300/1563], Loss: 0.0730\n",
            "Epoch [34/50], Step [400/1563], Loss: 0.0714\n",
            "Epoch [34/50], Step [500/1563], Loss: 0.0746\n",
            "Epoch [34/50], Step [600/1563], Loss: 0.0681\n",
            "Epoch [34/50], Step [700/1563], Loss: 0.0716\n",
            "Epoch [34/50], Step [800/1563], Loss: 0.0611\n",
            "Epoch [34/50], Step [900/1563], Loss: 0.0771\n",
            "Epoch [34/50], Step [1000/1563], Loss: 0.0657\n",
            "Epoch [34/50], Step [1100/1563], Loss: 0.0734\n",
            "Epoch [34/50], Step [1200/1563], Loss: 0.0727\n",
            "Epoch [34/50], Step [1300/1563], Loss: 0.0746\n",
            "Epoch [34/50], Step [1400/1563], Loss: 0.0810\n",
            "Epoch [34/50], Step [1500/1563], Loss: 0.0747\n",
            "Epoch [34/50], Train Loss: 0.0025, Val Loss: 0.3073, Accuracy: 91.08%\n",
            "Epoch [35/50], Step [100/1563], Loss: 0.0771\n",
            "Epoch [35/50], Step [200/1563], Loss: 0.0572\n",
            "Epoch [35/50], Step [300/1563], Loss: 0.0925\n",
            "Epoch [35/50], Step [400/1563], Loss: 0.0723\n",
            "Epoch [35/50], Step [500/1563], Loss: 0.0746\n",
            "Epoch [35/50], Step [600/1563], Loss: 0.0728\n",
            "Epoch [35/50], Step [700/1563], Loss: 0.0943\n",
            "Epoch [35/50], Step [800/1563], Loss: 0.0748\n",
            "Epoch [35/50], Step [900/1563], Loss: 0.0614\n",
            "Epoch [35/50], Step [1000/1563], Loss: 0.0646\n",
            "Epoch [35/50], Step [1100/1563], Loss: 0.0749\n",
            "Epoch [35/50], Step [1200/1563], Loss: 0.0572\n",
            "Epoch [35/50], Step [1300/1563], Loss: 0.0677\n",
            "Epoch [35/50], Step [1400/1563], Loss: 0.0798\n",
            "Epoch [35/50], Step [1500/1563], Loss: 0.0710\n",
            "Epoch [35/50], Train Loss: 0.0028, Val Loss: 0.3066, Accuracy: 91.08%\n",
            "Epoch [36/50], Step [100/1563], Loss: 0.0795\n",
            "Epoch [36/50], Step [200/1563], Loss: 0.0646\n",
            "Epoch [36/50], Step [300/1563], Loss: 0.0859\n",
            "Epoch [36/50], Step [400/1563], Loss: 0.0666\n",
            "Epoch [36/50], Step [500/1563], Loss: 0.0740\n",
            "Epoch [36/50], Step [600/1563], Loss: 0.0766\n",
            "Epoch [36/50], Step [700/1563], Loss: 0.0708\n",
            "Epoch [36/50], Step [800/1563], Loss: 0.0676\n",
            "Epoch [36/50], Step [900/1563], Loss: 0.0732\n",
            "Epoch [36/50], Step [1000/1563], Loss: 0.0663\n",
            "Epoch [36/50], Step [1100/1563], Loss: 0.0851\n",
            "Epoch [36/50], Step [1200/1563], Loss: 0.0743\n",
            "Epoch [36/50], Step [1300/1563], Loss: 0.0738\n",
            "Epoch [36/50], Step [1400/1563], Loss: 0.0617\n",
            "Epoch [36/50], Step [1500/1563], Loss: 0.0701\n",
            "Epoch [36/50], Train Loss: 0.0032, Val Loss: 0.3068, Accuracy: 91.00%\n",
            "Epoch [37/50], Step [100/1563], Loss: 0.0886\n",
            "Epoch [37/50], Step [200/1563], Loss: 0.0751\n",
            "Epoch [37/50], Step [300/1563], Loss: 0.0821\n",
            "Epoch [37/50], Step [400/1563], Loss: 0.0712\n",
            "Epoch [37/50], Step [500/1563], Loss: 0.0634\n",
            "Epoch [37/50], Step [600/1563], Loss: 0.0606\n",
            "Epoch [37/50], Step [700/1563], Loss: 0.0771\n",
            "Epoch [37/50], Step [800/1563], Loss: 0.0667\n",
            "Epoch [37/50], Step [900/1563], Loss: 0.0789\n",
            "Epoch [37/50], Step [1000/1563], Loss: 0.0793\n",
            "Epoch [37/50], Step [1100/1563], Loss: 0.0588\n",
            "Epoch [37/50], Step [1200/1563], Loss: 0.0721\n",
            "Epoch [37/50], Step [1300/1563], Loss: 0.0640\n",
            "Epoch [37/50], Step [1400/1563], Loss: 0.0660\n",
            "Epoch [37/50], Step [1500/1563], Loss: 0.0791\n",
            "Epoch [37/50], Train Loss: 0.0031, Val Loss: 0.3110, Accuracy: 91.02%\n",
            "Epoch [38/50], Step [100/1563], Loss: 0.0692\n",
            "Epoch [38/50], Step [200/1563], Loss: 0.0635\n",
            "Epoch [38/50], Step [300/1563], Loss: 0.0718\n",
            "Epoch [38/50], Step [400/1563], Loss: 0.0770\n",
            "Epoch [38/50], Step [500/1563], Loss: 0.0619\n",
            "Epoch [38/50], Step [600/1563], Loss: 0.0685\n",
            "Epoch [38/50], Step [700/1563], Loss: 0.0714\n",
            "Epoch [38/50], Step [800/1563], Loss: 0.0776\n",
            "Epoch [38/50], Step [900/1563], Loss: 0.0753\n",
            "Epoch [38/50], Step [1000/1563], Loss: 0.0706\n",
            "Epoch [38/50], Step [1100/1563], Loss: 0.0743\n",
            "Epoch [38/50], Step [1200/1563], Loss: 0.0836\n",
            "Epoch [38/50], Step [1300/1563], Loss: 0.0737\n",
            "Epoch [38/50], Step [1400/1563], Loss: 0.0716\n",
            "Epoch [38/50], Step [1500/1563], Loss: 0.0681\n",
            "Epoch [38/50], Train Loss: 0.0031, Val Loss: 0.3085, Accuracy: 90.79%\n",
            "Epoch [39/50], Step [100/1563], Loss: 0.0738\n",
            "Epoch [39/50], Step [200/1563], Loss: 0.0774\n",
            "Epoch [39/50], Step [300/1563], Loss: 0.0607\n",
            "Epoch [39/50], Step [400/1563], Loss: 0.0596\n",
            "Epoch [39/50], Step [500/1563], Loss: 0.0698\n",
            "Epoch [39/50], Step [600/1563], Loss: 0.0813\n",
            "Epoch [39/50], Step [700/1563], Loss: 0.0739\n",
            "Epoch [39/50], Step [800/1563], Loss: 0.0781\n",
            "Epoch [39/50], Step [900/1563], Loss: 0.0646\n",
            "Epoch [39/50], Step [1000/1563], Loss: 0.0648\n",
            "Epoch [39/50], Step [1100/1563], Loss: 0.0684\n",
            "Epoch [39/50], Step [1200/1563], Loss: 0.0730\n",
            "Epoch [39/50], Step [1300/1563], Loss: 0.0696\n",
            "Epoch [39/50], Step [1400/1563], Loss: 0.0723\n",
            "Epoch [39/50], Step [1500/1563], Loss: 0.0698\n",
            "Epoch [39/50], Train Loss: 0.0032, Val Loss: 0.3070, Accuracy: 90.95%\n",
            "Epoch [40/50], Step [100/1563], Loss: 0.0736\n",
            "Epoch [40/50], Step [200/1563], Loss: 0.0690\n",
            "Epoch [40/50], Step [300/1563], Loss: 0.0804\n",
            "Epoch [40/50], Step [400/1563], Loss: 0.0732\n",
            "Epoch [40/50], Step [500/1563], Loss: 0.0780\n",
            "Epoch [40/50], Step [600/1563], Loss: 0.0825\n",
            "Epoch [40/50], Step [700/1563], Loss: 0.0701\n",
            "Epoch [40/50], Step [800/1563], Loss: 0.0747\n",
            "Epoch [40/50], Step [900/1563], Loss: 0.0697\n",
            "Epoch [40/50], Step [1000/1563], Loss: 0.0635\n",
            "Epoch [40/50], Step [1100/1563], Loss: 0.0762\n",
            "Epoch [40/50], Step [1200/1563], Loss: 0.0836\n",
            "Epoch [40/50], Step [1300/1563], Loss: 0.0684\n",
            "Epoch [40/50], Step [1400/1563], Loss: 0.0730\n",
            "Epoch [40/50], Step [1500/1563], Loss: 0.0868\n",
            "Epoch [40/50], Train Loss: 0.0029, Val Loss: 0.3087, Accuracy: 91.09%\n",
            "Epoch [41/50], Step [100/1563], Loss: 0.0653\n",
            "Epoch [41/50], Step [200/1563], Loss: 0.0726\n",
            "Epoch [41/50], Step [300/1563], Loss: 0.0738\n",
            "Epoch [41/50], Step [400/1563], Loss: 0.0863\n",
            "Epoch [41/50], Step [500/1563], Loss: 0.0706\n",
            "Epoch [41/50], Step [600/1563], Loss: 0.0670\n",
            "Epoch [41/50], Step [700/1563], Loss: 0.0660\n",
            "Epoch [41/50], Step [800/1563], Loss: 0.0735\n",
            "Epoch [41/50], Step [900/1563], Loss: 0.0626\n",
            "Epoch [41/50], Step [1000/1563], Loss: 0.0733\n",
            "Epoch [41/50], Step [1100/1563], Loss: 0.0677\n",
            "Epoch [41/50], Step [1200/1563], Loss: 0.0746\n",
            "Epoch [41/50], Step [1300/1563], Loss: 0.0811\n",
            "Epoch [41/50], Step [1400/1563], Loss: 0.0778\n",
            "Epoch [41/50], Step [1500/1563], Loss: 0.0675\n",
            "Epoch [41/50], Train Loss: 0.0034, Val Loss: 0.3072, Accuracy: 91.14%\n",
            "Epoch [42/50], Step [100/1563], Loss: 0.0630\n",
            "Epoch [42/50], Step [200/1563], Loss: 0.0633\n",
            "Epoch [42/50], Step [300/1563], Loss: 0.0802\n",
            "Epoch [42/50], Step [400/1563], Loss: 0.0665\n",
            "Epoch [42/50], Step [500/1563], Loss: 0.0718\n",
            "Epoch [42/50], Step [600/1563], Loss: 0.0798\n",
            "Epoch [42/50], Step [700/1563], Loss: 0.0845\n",
            "Epoch [42/50], Step [800/1563], Loss: 0.0751\n",
            "Epoch [42/50], Step [900/1563], Loss: 0.0763\n",
            "Epoch [42/50], Step [1000/1563], Loss: 0.0847\n",
            "Epoch [42/50], Step [1100/1563], Loss: 0.0657\n",
            "Epoch [42/50], Step [1200/1563], Loss: 0.0678\n",
            "Epoch [42/50], Step [1300/1563], Loss: 0.0666\n",
            "Epoch [42/50], Step [1400/1563], Loss: 0.0670\n",
            "Epoch [42/50], Step [1500/1563], Loss: 0.0777\n",
            "Epoch [42/50], Train Loss: 0.0026, Val Loss: 0.3057, Accuracy: 91.15%\n",
            "Epoch [43/50], Step [100/1563], Loss: 0.0742\n",
            "Epoch [43/50], Step [200/1563], Loss: 0.0680\n",
            "Epoch [43/50], Step [300/1563], Loss: 0.0694\n",
            "Epoch [43/50], Step [400/1563], Loss: 0.0640\n",
            "Epoch [43/50], Step [500/1563], Loss: 0.0684\n",
            "Epoch [43/50], Step [600/1563], Loss: 0.0705\n",
            "Epoch [43/50], Step [700/1563], Loss: 0.0882\n",
            "Epoch [43/50], Step [800/1563], Loss: 0.0784\n",
            "Epoch [43/50], Step [900/1563], Loss: 0.0660\n",
            "Epoch [43/50], Step [1000/1563], Loss: 0.0696\n",
            "Epoch [43/50], Step [1100/1563], Loss: 0.0924\n",
            "Epoch [43/50], Step [1200/1563], Loss: 0.0700\n",
            "Epoch [43/50], Step [1300/1563], Loss: 0.0672\n",
            "Epoch [43/50], Step [1400/1563], Loss: 0.0693\n",
            "Epoch [43/50], Step [1500/1563], Loss: 0.0737\n",
            "Epoch [43/50], Train Loss: 0.0028, Val Loss: 0.3080, Accuracy: 91.01%\n",
            "Epoch [44/50], Step [100/1563], Loss: 0.0687\n",
            "Epoch [44/50], Step [200/1563], Loss: 0.0857\n",
            "Epoch [44/50], Step [300/1563], Loss: 0.0674\n",
            "Epoch [44/50], Step [400/1563], Loss: 0.0695\n",
            "Epoch [44/50], Step [500/1563], Loss: 0.0721\n",
            "Epoch [44/50], Step [600/1563], Loss: 0.0689\n",
            "Epoch [44/50], Step [700/1563], Loss: 0.0823\n",
            "Epoch [44/50], Step [800/1563], Loss: 0.0685\n",
            "Epoch [44/50], Step [900/1563], Loss: 0.0763\n",
            "Epoch [44/50], Step [1000/1563], Loss: 0.0626\n",
            "Epoch [44/50], Step [1100/1563], Loss: 0.0834\n",
            "Epoch [44/50], Step [1200/1563], Loss: 0.0728\n",
            "Epoch [44/50], Step [1300/1563], Loss: 0.0674\n",
            "Epoch [44/50], Step [1400/1563], Loss: 0.0636\n",
            "Epoch [44/50], Step [1500/1563], Loss: 0.0687\n",
            "Epoch [44/50], Train Loss: 0.0028, Val Loss: 0.3074, Accuracy: 90.98%\n",
            "Epoch [45/50], Step [100/1563], Loss: 0.0777\n",
            "Epoch [45/50], Step [200/1563], Loss: 0.0664\n",
            "Epoch [45/50], Step [300/1563], Loss: 0.0735\n",
            "Epoch [45/50], Step [400/1563], Loss: 0.0740\n",
            "Epoch [45/50], Step [500/1563], Loss: 0.0698\n",
            "Epoch [45/50], Step [600/1563], Loss: 0.0760\n",
            "Epoch [45/50], Step [700/1563], Loss: 0.0739\n",
            "Epoch [45/50], Step [800/1563], Loss: 0.0682\n",
            "Epoch [45/50], Step [900/1563], Loss: 0.0651\n",
            "Epoch [45/50], Step [1000/1563], Loss: 0.0632\n",
            "Epoch [45/50], Step [1100/1563], Loss: 0.0693\n",
            "Epoch [45/50], Step [1200/1563], Loss: 0.0721\n",
            "Epoch [45/50], Step [1300/1563], Loss: 0.0699\n",
            "Epoch [45/50], Step [1400/1563], Loss: 0.0694\n",
            "Epoch [45/50], Step [1500/1563], Loss: 0.0757\n",
            "Epoch [45/50], Train Loss: 0.0032, Val Loss: 0.3119, Accuracy: 91.15%\n",
            "Epoch [46/50], Step [100/1563], Loss: 0.0622\n",
            "Epoch [46/50], Step [200/1563], Loss: 0.0770\n",
            "Epoch [46/50], Step [300/1563], Loss: 0.0786\n",
            "Epoch [46/50], Step [400/1563], Loss: 0.0845\n",
            "Epoch [46/50], Step [500/1563], Loss: 0.0658\n",
            "Epoch [46/50], Step [600/1563], Loss: 0.0724\n",
            "Epoch [46/50], Step [700/1563], Loss: 0.0668\n",
            "Epoch [46/50], Step [800/1563], Loss: 0.0778\n",
            "Epoch [46/50], Step [900/1563], Loss: 0.0757\n",
            "Epoch [46/50], Step [1000/1563], Loss: 0.0742\n",
            "Epoch [46/50], Step [1100/1563], Loss: 0.0658\n",
            "Epoch [46/50], Step [1200/1563], Loss: 0.0741\n",
            "Epoch [46/50], Step [1300/1563], Loss: 0.0691\n",
            "Epoch [46/50], Step [1400/1563], Loss: 0.0633\n",
            "Epoch [46/50], Step [1500/1563], Loss: 0.0713\n",
            "Epoch [46/50], Train Loss: 0.0030, Val Loss: 0.3055, Accuracy: 90.99%\n",
            "Epoch [47/50], Step [100/1563], Loss: 0.0832\n",
            "Epoch [47/50], Step [200/1563], Loss: 0.0751\n",
            "Epoch [47/50], Step [300/1563], Loss: 0.0698\n",
            "Epoch [47/50], Step [400/1563], Loss: 0.0633\n",
            "Epoch [47/50], Step [500/1563], Loss: 0.0737\n",
            "Epoch [47/50], Step [600/1563], Loss: 0.0656\n",
            "Epoch [47/50], Step [700/1563], Loss: 0.0750\n",
            "Epoch [47/50], Step [800/1563], Loss: 0.0648\n",
            "Epoch [47/50], Step [900/1563], Loss: 0.0672\n",
            "Epoch [47/50], Step [1000/1563], Loss: 0.0599\n",
            "Epoch [47/50], Step [1100/1563], Loss: 0.0620\n",
            "Epoch [47/50], Step [1200/1563], Loss: 0.0810\n",
            "Epoch [47/50], Step [1300/1563], Loss: 0.0819\n",
            "Epoch [47/50], Step [1400/1563], Loss: 0.0649\n",
            "Epoch [47/50], Step [1500/1563], Loss: 0.0660\n",
            "Epoch [47/50], Train Loss: 0.0029, Val Loss: 0.3048, Accuracy: 91.12%\n",
            "Epoch [48/50], Step [100/1563], Loss: 0.0688\n",
            "Epoch [48/50], Step [200/1563], Loss: 0.0692\n",
            "Epoch [48/50], Step [300/1563], Loss: 0.0798\n",
            "Epoch [48/50], Step [400/1563], Loss: 0.0678\n",
            "Epoch [48/50], Step [500/1563], Loss: 0.0733\n",
            "Epoch [48/50], Step [600/1563], Loss: 0.0701\n",
            "Epoch [48/50], Step [700/1563], Loss: 0.0787\n",
            "Epoch [48/50], Step [800/1563], Loss: 0.0676\n",
            "Epoch [48/50], Step [900/1563], Loss: 0.0686\n",
            "Epoch [48/50], Step [1000/1563], Loss: 0.0732\n",
            "Epoch [48/50], Step [1100/1563], Loss: 0.0705\n",
            "Epoch [48/50], Step [1200/1563], Loss: 0.0756\n",
            "Epoch [48/50], Step [1300/1563], Loss: 0.0748\n",
            "Epoch [48/50], Step [1400/1563], Loss: 0.0684\n",
            "Epoch [48/50], Step [1500/1563], Loss: 0.0675\n",
            "Epoch [48/50], Train Loss: 0.0031, Val Loss: 0.3087, Accuracy: 91.01%\n",
            "Epoch [49/50], Step [100/1563], Loss: 0.0810\n",
            "Epoch [49/50], Step [200/1563], Loss: 0.0728\n",
            "Epoch [49/50], Step [300/1563], Loss: 0.0704\n",
            "Epoch [49/50], Step [400/1563], Loss: 0.0663\n",
            "Epoch [49/50], Step [500/1563], Loss: 0.0647\n",
            "Epoch [49/50], Step [600/1563], Loss: 0.0649\n",
            "Epoch [49/50], Step [700/1563], Loss: 0.0685\n",
            "Epoch [49/50], Step [800/1563], Loss: 0.0682\n",
            "Epoch [49/50], Step [900/1563], Loss: 0.0760\n",
            "Epoch [49/50], Step [1000/1563], Loss: 0.0736\n",
            "Epoch [49/50], Step [1100/1563], Loss: 0.0734\n",
            "Epoch [49/50], Step [1200/1563], Loss: 0.0804\n",
            "Epoch [49/50], Step [1300/1563], Loss: 0.0704\n",
            "Epoch [49/50], Step [1400/1563], Loss: 0.0697\n",
            "Epoch [49/50], Step [1500/1563], Loss: 0.0616\n",
            "Epoch [49/50], Train Loss: 0.0029, Val Loss: 0.3098, Accuracy: 91.07%\n",
            "Epoch [50/50], Step [100/1563], Loss: 0.0665\n",
            "Epoch [50/50], Step [200/1563], Loss: 0.0736\n",
            "Epoch [50/50], Step [300/1563], Loss: 0.0686\n",
            "Epoch [50/50], Step [400/1563], Loss: 0.0788\n",
            "Epoch [50/50], Step [500/1563], Loss: 0.0632\n",
            "Epoch [50/50], Step [600/1563], Loss: 0.0769\n",
            "Epoch [50/50], Step [700/1563], Loss: 0.0817\n",
            "Epoch [50/50], Step [800/1563], Loss: 0.0699\n",
            "Epoch [50/50], Step [900/1563], Loss: 0.0683\n",
            "Epoch [50/50], Step [1000/1563], Loss: 0.0670\n",
            "Epoch [50/50], Step [1100/1563], Loss: 0.0797\n",
            "Epoch [50/50], Step [1200/1563], Loss: 0.0848\n",
            "Epoch [50/50], Step [1300/1563], Loss: 0.0633\n",
            "Epoch [50/50], Step [1400/1563], Loss: 0.0766\n",
            "Epoch [50/50], Step [1500/1563], Loss: 0.0817\n",
            "Epoch [50/50], Train Loss: 0.0031, Val Loss: 0.3072, Accuracy: 90.90%\n",
            "Best Test Accuracy: 91.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"alexNet.pth\")"
      ],
      "metadata": {
        "id": "4M9Uar1bJ4ek"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}